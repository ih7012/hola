{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyuOCYM92LJb"
      },
      "source": [
        "# Getting Started with Fine-Tuning Moshi 7B\n",
        "\n",
        "This notebook shows you a simple example of how to LoRA finetune Moshi 7B. You can run this notebook in Google Colab using a A100 GPU.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github//kyutai-labs/moshi-finetune/blob/main/tutorials/moshi_finetune.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Check out `moshi-finetune` Github repo to learn more: https://github.com/kyutai-labs/moshi-finetune/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxr8mv-17GfB"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Clone the `moshi-finetune` repo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj3IlIeVDIb",
        "outputId": "ff67c114-223b-46b0-aa2d-5fe61dbbdc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'moshi-finetune'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 245 (delta 36), reused 35 (delta 29), pack-reused 190 (from 1)\u001b[K\n",
            "Receiving objects: 100% (245/245), 638.97 KiB | 13.31 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/kyutai-labs/moshi-finetune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPd_pGT7WiY"
      },
      "source": [
        "Install all required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuTOGipl7BS7"
      },
      "outputs": [],
      "source": [
        "%pip install -e /content/moshi-finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ams-19wF8zgY"
      },
      "source": [
        "## Prepare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSesbUVtp2sz"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "Path(\"/content/data/daily-talk-contiguous\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "local_dir = snapshot_download(\n",
        "    \"kyutai/DailyTalkContiguous\",\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=\"/content/data/daily-talk-contiguous\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hia7n0T1_mHZ"
      },
      "source": [
        "## Start training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZtcLerooWFeB"
      },
      "outputs": [],
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5dxTlIQMaJGv"
      },
      "outputs": [],
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "import yaml\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "data:\n",
        "  train_data: '/content/data/daily-talk-contiguous/dailytalk.jsonl' # Fill\n",
        "  eval_data: '' # Optionally Fill\n",
        "  shuffle: true\n",
        "\n",
        "# model\n",
        "moshi_paths:\n",
        "  hf_repo_id: \"kyutai/moshiko-pytorch-bf16\"\n",
        "\n",
        "\n",
        "full_finetuning: false # Activate lora.enable if partial finetuning\n",
        "lora:\n",
        "  enable: true\n",
        "  rank: 128\n",
        "  scaling: 2.\n",
        "  ft_embed: false\n",
        "\n",
        "# training hyperparameters\n",
        "first_codebook_weight_multiplier: 100.\n",
        "text_padding_weight: .5\n",
        "\n",
        "\n",
        "# tokens per training steps = batch_size x num_GPUs x duration_sec\n",
        "# we recommend a sequence duration of 300 seconds\n",
        "# If you run into memory error, you can try reduce the sequence length\n",
        "duration_sec: 50\n",
        "batch_size: 1\n",
        "max_steps: 30\n",
        "\n",
        "gradient_checkpointing: true # Activate checkpointing of layers\n",
        "\n",
        "# optim\n",
        "optim:\n",
        "  lr: 2.e-6\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 10\n",
        "eval_freq: 1\n",
        "do_eval: False\n",
        "ckpt_freq: 10\n",
        "\n",
        "save_adapters: True\n",
        "\n",
        "run_dir: \"/content/test\"  # Fill\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "with open(\"/content/example.yaml\", \"w\") as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErD1ktQUMyPZ"
      },
      "outputs": [],
      "source": [
        "# make sure the run_dir has not been created before\n",
        "# only run this when you ran torchrun previously and created the /content/test_ultra file\n",
        "# ! rm -r /content/test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training"
      ],
      "metadata": {
        "id": "sH4koaJUsTlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Imports\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import pprint\n",
        "import shutil\n",
        "from contextlib import ExitStack\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.distributed as dist\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "\n",
        "from finetune.args import TrainArgs\n",
        "from finetune.checkpointing import Checkpointer\n",
        "from finetune.data.data_loader import build_data_loader\n",
        "from finetune.data.interleaver import InterleavedTokenizer, Interleaver\n",
        "from finetune.distributed import (\n",
        "    BACKEND,\n",
        "    avg_aggregate,\n",
        "    get_rank,\n",
        "    get_world_size,\n",
        "    is_torchrun,\n",
        "    set_device,\n",
        ")\n",
        "from finetune.eval import evaluate\n",
        "from finetune.loss import compute_loss_with_mask\n",
        "from finetune.mixed_precision import (\n",
        "    downcast_mixed_precision,\n",
        "    prepare_mixed_precision,\n",
        "    upcast_mixed_precision,\n",
        ")\n",
        "from finetune.monitoring.metrics_logger import (\n",
        "    MetricsLogger,\n",
        "    eval_log_msg,\n",
        "    get_eval_logs,\n",
        "    get_train_logs,\n",
        "    train_log_msg,\n",
        ")\n",
        "from finetune.monitoring.utils import set_logger\n",
        "from finetune.utils import TrainState, logged_closing, set_random_seed\n",
        "from finetune.wrapped_model import get_fsdp_model\n",
        "from moshi.models import loaders\n",
        "\n",
        "logger = logging.getLogger(\"train\")"
      ],
      "metadata": {
        "id": "R4m5AUAEsZGy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - Utility logging function\n",
        "def main_logger_info(message: str) -> None:\n",
        "    if get_rank() == 0:\n",
        "        logger.info(message)"
      ],
      "metadata": {
        "id": "fJldy_hisg-2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 - Entry point\n",
        "def train(config: str):\n",
        "    args: TrainArgs = TrainArgs.load(config, drop_extra_fields=False)\n",
        "    set_logger(logging.INFO)\n",
        "\n",
        "    with ExitStack() as exit_stack:\n",
        "        _train(args, exit_stack)\n",
        "    logger.info(\"Closed everything!\")"
      ],
      "metadata": {
        "id": "UNlXyxFnsi3I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _train(args: TrainArgs, exit_stack: ExitStack):\n",
        "    set_random_seed(args.seed)\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "    # Init NCCL\n",
        "    if \"LOCAL_RANK\" in os.environ:\n",
        "        set_device()\n",
        "        logger.info(\"Going to init comms...\")\n",
        "        dist.init_process_group(backend=BACKEND)\n",
        "    else:\n",
        "        logger.error(\"PyTorch environment is not correctly initialized.\")\n",
        "\n",
        "    # Init run dir\n",
        "    main_logger_info(f\"Run dir: {args.run_dir}\")\n",
        "    run_dir = Path(args.run_dir)\n",
        "\n",
        "    if is_torchrun():\n",
        "        if run_dir.exists() and not args.overwrite_run_dir:\n",
        "            raise RuntimeError(f\"Run dir {run_dir} already exists.\")\n",
        "        elif run_dir.exists():\n",
        "            main_logger_info(f\"Removing run dir {run_dir}...\")\n",
        "            shutil.rmtree(run_dir)\n",
        "\n",
        "    if args.full_finetuning:\n",
        "        assert not args.lora.enable, \"LoRA should not be enabled for full finetuning.\"\n",
        "    else:\n",
        "        assert args.lora.enable, \"LoRA should be enabled for partial finetuning\"\n",
        "\n",
        "    dist.barrier()\n",
        "    run_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    args_path = run_dir / \"args.yaml\"\n",
        "    if not args_path.exists():\n",
        "        args.save(args_path)\n",
        "\n",
        "    main_logger_info(f\"TrainArgs: {pprint.pformat(dataclasses.asdict(args))}\")\n",
        "\n",
        "    # Loggers\n",
        "    metrics_logger = MetricsLogger(\n",
        "        run_dir, \"train\", get_rank() == 0, args.wandb, dataclasses.asdict(args)\n",
        "    )\n",
        "    exit_stack.enter_context(logged_closing(metrics_logger, \"metrics_logger\"))\n",
        "\n",
        "    eval_logger = MetricsLogger(\n",
        "        run_dir, \"eval\", get_rank() == 0, args.wandb, dataclasses.asdict(args)\n",
        "    )\n",
        "    exit_stack.enter_context(logged_closing(eval_logger, \"eval_logger\"))\n",
        "\n",
        "    # Load models\n",
        "    main_logger_info(\"Loading Mimi and Moshi...\")\n",
        "    checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\n",
        "        hf_repo=args.moshi_paths.hf_repo_id,\n",
        "        moshi_weights=args.moshi_paths.moshi_path,\n",
        "        mimi_weights=args.moshi_paths.mimi_path,\n",
        "        tokenizer=args.moshi_paths.tokenizer_path,\n",
        "        config_path=args.moshi_paths.config_path,\n",
        "    )\n",
        "\n",
        "    lm_config = (\n",
        "        loaders._lm_kwargs if checkpoint_info.raw_config is None else checkpoint_info.raw_config\n",
        "    )\n",
        "    lm_config[\"lora\"] = args.lora.enable\n",
        "    lm_config[\"lora_rank\"] = args.lora.rank\n",
        "    lm_config[\"lora_scaling\"] = args.lora.scaling\n",
        "\n",
        "    mimi = checkpoint_info.get_mimi(device=\"cuda\")\n",
        "    mimi.eval()\n",
        "    for p in mimi.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    model = get_fsdp_model(args, checkpoint_info)\n",
        "\n",
        "    spm = checkpoint_info.get_text_tokenizer()\n",
        "\n",
        "    interleaver = Interleaver(\n",
        "        spm,\n",
        "        mimi.frame_rate,\n",
        "        model.text_padding_token_id,\n",
        "        model.end_of_text_padding_id,\n",
        "        model.zero_token_id,\n",
        "        keep_main_only=True,\n",
        "    )\n",
        "    interleaved_tokenizer = InterleavedTokenizer(\n",
        "        mimi, interleaver, duration_sec=args.duration_sec\n",
        "    )\n",
        "\n",
        "    # Data loaders\n",
        "    data_loader = build_data_loader(\n",
        "        instruct_tokenizer=interleaved_tokenizer,\n",
        "        args=args.data,\n",
        "        batch_size=args.batch_size,\n",
        "        seed=args.seed,\n",
        "        rank=get_rank(),\n",
        "        world_size=get_world_size(),\n",
        "        is_eval=False,\n",
        "    )\n",
        "\n",
        "    if args.do_eval:\n",
        "        eval_data_loader = build_data_loader(\n",
        "            instruct_tokenizer=interleaved_tokenizer,\n",
        "            args=args.data,\n",
        "            batch_size=args.batch_size,\n",
        "            seed=None,\n",
        "            rank=get_rank(),\n",
        "            world_size=get_world_size(),\n",
        "            is_eval=True,\n",
        "        )\n",
        "\n",
        "    # Optimizer / Scheduler\n",
        "    param_dtype = getattr(torch, args.param_dtype)\n",
        "    optim_dtype = torch.float32\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=args.optim.lr,\n",
        "        betas=(0.9, 0.95),\n",
        "        eps=1e-08,\n",
        "        weight_decay=args.optim.weight_decay,\n",
        "    )\n",
        "    scheduler = lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=args.optim.lr,\n",
        "        total_steps=args.max_steps,\n",
        "        pct_start=args.optim.pct_start,\n",
        "    )\n",
        "\n",
        "    state = TrainState(args.max_steps)\n",
        "\n",
        "    # Checkpointer\n",
        "    if args.do_ckpt:\n",
        "        checkpointer = Checkpointer(\n",
        "            model=model,\n",
        "            state=state,\n",
        "            config=lm_config,\n",
        "            run_dir=run_dir,\n",
        "            optimizer=optimizer,\n",
        "            num_ckpt_keep=args.num_ckpt_keep,\n",
        "            full_finetuning=args.full_finetuning,\n",
        "        )\n",
        "\n",
        "    # Mixed precision\n",
        "    prepare_mixed_precision(\n",
        "        model.parameters(), param_dtype=param_dtype, optim_dtype=optim_dtype\n",
        "    )\n",
        "\n",
        "    # Train loop\n",
        "    model.train()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    while state.step < args.max_steps:\n",
        "        state.start_step()\n",
        "        is_last_step = state.step == args.max_steps\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.tensor([0.0], device=\"cuda\")\n",
        "        n_batch_tokens = 0\n",
        "        n_real_tokens = 0\n",
        "\n",
        "        for i in range(args.num_microbatches):\n",
        "            batch = next(data_loader)\n",
        "            codes = batch.codes\n",
        "\n",
        "            condition_tensors = None\n",
        "            if batch.condition_attributes is not None:\n",
        "                condition_tensors = model.condition_provider.prepare(\n",
        "                    batch.condition_attributes\n",
        "                )\n",
        "\n",
        "            output = model(codes=codes, condition_tensors=condition_tensors)\n",
        "            text_loss = compute_loss_with_mask(\n",
        "                output.text_logits,\n",
        "                codes[:, : model.audio_offset],\n",
        "                output.text_mask,\n",
        "                mode=\"text\",\n",
        "                text_padding_weight=args.text_padding_weight,\n",
        "                text_padding_ids={\n",
        "                    model.text_padding_token_id,\n",
        "                    model.end_of_text_padding_id,\n",
        "                },\n",
        "            )\n",
        "            audio_loss = compute_loss_with_mask(\n",
        "                output.logits,\n",
        "                codes[:, model.audio_offset : model.audio_offset + model.dep_q],\n",
        "                output.mask,\n",
        "                mode=\"audio\",\n",
        "                first_codebook_weight_multiplier=args.first_codebook_weight_multiplier,\n",
        "            )\n",
        "\n",
        "            mb_loss = text_loss + audio_loss\n",
        "            mb_loss.backward()\n",
        "\n",
        "            loss += mb_loss.detach()\n",
        "            n_batch_tokens += output.text_mask.numel() + output.mask.numel()\n",
        "            n_real_tokens += (\n",
        "                torch.sum(output.text_mask).item() + torch.sum(output.mask).item()\n",
        "            )\n",
        "\n",
        "            if i < args.num_microbatches - 1:\n",
        "                assert args.num_microbatches > 1\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "        if args.num_microbatches > 1:\n",
        "            loss /= args.num_microbatches\n",
        "            for p in model.parameters():\n",
        "                if p.requires_grad:\n",
        "                    assert p.grad is not None\n",
        "                    p.grad.div_(args.num_microbatches)\n",
        "\n",
        "        upcast_mixed_precision(model.parameters(), optim_dtype=optim_dtype)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
        "        optimizer.step()\n",
        "        downcast_mixed_precision(model.parameters(), param_dtype=param_dtype)\n",
        "\n",
        "        last_lr = scheduler.get_last_lr()[0]\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_loss = avg_aggregate(loss.item())\n",
        "\n",
        "        if args.do_eval and (\n",
        "            (args.eval_freq > 0 and state.step % args.eval_freq == 0) or is_last_step\n",
        "        ):\n",
        "            evaluate(model, eval_data_loader, state, args)\n",
        "            eval_logs = get_eval_logs(\n",
        "                state.step, avg_loss, state.this_eval_perplexity, state.this_eval_loss\n",
        "            )\n",
        "            main_logger_info(eval_log_msg(eval_logs))\n",
        "            eval_logger.log(eval_logs, step=state.step)\n",
        "\n",
        "        state.end_step(n_batch_tokens)\n",
        "\n",
        "        if state.step % args.log_freq == 0:\n",
        "            train_logs = get_train_logs(\n",
        "                state,\n",
        "                avg_loss,\n",
        "                n_real_tokens,\n",
        "                last_lr,\n",
        "                torch.cuda.max_memory_allocated(),\n",
        "                torch.cuda.memory_allocated(),\n",
        "                args,\n",
        "            )\n",
        "            main_logger_info(train_log_msg(state, logs=train_logs, loss=avg_loss))\n",
        "            metrics_logger.log(train_logs, step=state.step)\n",
        "\n",
        "        if args.do_ckpt and (\n",
        "            (args.ckpt_freq > 0 and state.step % args.ckpt_freq == 0) or is_last_step\n",
        "        ):\n",
        "            checkpointer.save_checkpoint(\n",
        "                save_only_lora=not args.full_finetuning and args.save_adapters,\n",
        "                dtype=param_dtype,\n",
        "            )\n",
        "\n",
        "    main_logger_info(\"done!\")"
      ],
      "metadata": {
        "id": "Cx64HR6fsqdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4wFgmwIUTtg"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "\n",
        "# !cd /content/moshi-finetune && torchrun --nproc-per-node 1 -m train /content/example.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruJ29JFn98zE"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Once the model has been trained, inference can be run on the colab GPU too, and gradio can be used to tunnel the audio data from a local client to the notebook.\n",
        "\n",
        "More details on how to set this up can be found in the [moshi readme](https://github.com/kyutai-labs/moshi?tab=readme-ov-file#python-pytorch).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yR3u9sWp2s8"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-xLs2Ot9-il"
      },
      "outputs": [],
      "source": [
        "!python -m moshi.server --gradio-tunnel --lora-weight=/content/test/checkpoints/checkpoint_000300/consolidated/lora.safetensors --config-path=/content/test/checkpoints/checkpoint_000300/consolidated/config.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}